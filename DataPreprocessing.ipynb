{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl (4.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.0MB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/e5/af35f7ea75cf72f2cd079c95ee16797de7cd71f29ea7c68ae5ce7be1eda0/PyYAML-6.0.1.tar.gz (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/df/1b454741459f6ce75f86534bdad42ca17291b14a83066695f7d2c676e16c/huggingface_hub-0.4.0-py3-none-any.whl (67kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.4MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/57/da0cb8e40437f88630769164a66afec8af294ff686661497b6c88bc08556/tokenizers-0.12.1.tar.gz (220kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from transformers) (2.19.1)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/ce/8916d10ef537f3f3b046843255f9799504aa41862bfa87844b9bdc5361cd/filelock-3.4.1-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/75/ed64d258199c5ea90be0b6f772a7e3d408e794f54236afa0f28e1db384d6/regex-2023.8.8-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (678kB)\n",
      "\u001b[K    100% |████████████████████████████████| 686kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/78/fef8d089db5b97546fd6d1ff2e813b8544e85670bf3a8c378c9d0250b98d/sacremoses-0.0.53.tar.gz (880kB)\n",
      "\u001b[K    100% |████████████████████████████████| 880kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\" (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/a1/b153a0a4caf7a7e3f15c2cd56c7702e2cf3d89b1b359d1f1c5e59d68f4ce/importlib_metadata-4.8.3-py3-none-any.whl\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.1.0->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Collecting importlib-resources; python_version < \"3.7\" (from tqdm>=4.27->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/24/1b/33e489669a94da3ef4562938cd306e8fa915e13939d7b8277cb5569cb405/importlib_resources-5.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (2018.8.24)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Collecting click (from sacremoses->transformers)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/a8/0b2ced25639fb20cc1c9784de90a8c25f9504a7f18cd8b5397bd61696d7d/click-8.0.4-py3-none-any.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 3.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting joblib (from sacremoses->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/91/d3ba0401e62d7e42816bc7d97b82d19c95c164b3e149a87c0a1c026a735e/joblib-1.1.1-py2.py3-none-any.whl (309kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.2.0)\n",
      "Building wheels for collected packages: pyyaml, tokenizers, sacremoses\n",
      "  Running setup.py bdist_wheel for pyyaml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nxp330/.cache/pip/wheels/e6/c6/ef/4e8ee93f1b79fc90562f1600d47189799f8213023d9dadafa2\n",
      "  Running setup.py bdist_wheel for tokenizers ... \u001b[?25lerror\n",
      "  Complete output from command /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/tmp/job.19121190.hpc/pip-install-9sw7bo4k/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/job.19121190.hpc/pip-wheel-1lgwsmqe --python-tag cp36:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.6\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for tokenizers\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Complete output from command /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/tmp/job.19121190.hpc/pip-install-9sw7bo4k/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" clean --all:\n",
      "  running clean\n",
      "  removing 'build/lib.linux-x86_64-3.6' (and everything under it)\n",
      "  'build/bdist.linux-x86_64' does not exist -- can't clean it\n",
      "  'build/scripts-3.6' does not exist -- can't clean it\n",
      "  removing 'build'\n",
      "  running clean_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed cleaning build dir for tokenizers\u001b[0m\n",
      "  Running setup.py bdist_wheel for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nxp330/.cache/pip/wheels/56/d5/b2/bc878b2bbddfbcc8fd62ca73c4fd842bd28c1fd3dbdf424c74\n",
      "Successfully built pyyaml sacremoses\n",
      "Failed to build tokenizers\n",
      "\u001b[31mtensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-gpu 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyyaml, filelock, typing-extensions, zipp, importlib-metadata, importlib-resources, tqdm, packaging, huggingface-hub, numpy, tokenizers, regex, click, joblib, sacremoses, dataclasses, transformers\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages/PyYAML-3.13.dist-info/INSTALLER'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.0, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/fd/75/ed64d258199c5ea90be0b6f772a7e3d408e794f54236afa0f28e1db384d6/regex-2023.8.8-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/84/ce/8916d10ef537f3f3b046843255f9799504aa41862bfa87844b9bdc5361cd/filelock-3.4.1-py3-none-any.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/a1/b153a0a4caf7a7e3f15c2cd56c7702e2cf3d89b1b359d1f1c5e59d68f4ce/importlib_metadata-4.8.3-py3-none-any.whl\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/df/1b454741459f6ce75f86534bdad42ca17291b14a83066695f7d2c676e16c/huggingface_hub-0.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from transformers) (2.19.1)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/57/da0cb8e40437f88630769164a66afec8af294ff686661497b6c88bc08556/tokenizers-0.12.1.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Collecting sacremoses (from transformers)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl\n",
      "Collecting importlib-resources; python_version < \"3.7\" (from tqdm>=4.27->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/24/1b/33e489669a94da3ef4562938cd306e8fa915e13939d7b8277cb5569cb405/importlib_resources-5.4.0-py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/df/d4a4974a3e3957fd1c1fa3082366d7fff6e428ddb55f074bf64876f8e8ad/zipp-3.6.0-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from requests->transformers) (2.7)\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/7c/91/d3ba0401e62d7e42816bc7d97b82d19c95c164b3e149a87c0a1c026a735e/joblib-1.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from sacremoses->transformers) (1.11.0)\n",
      "Collecting click (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/a8/0b2ced25639fb20cc1c9784de90a8c25f9504a7f18cd8b5397bd61696d7d/click-8.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.2.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Running setup.py bdist_wheel for tokenizers ... \u001b[?25lerror\n",
      "  Complete output from command /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/tmp/job.19121190.hpc/pip-install-xdcpgga5/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/job.19121190.hpc/pip-wheel-b9xmnvt3 --python-tag cp36:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.6\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-3.6/tokenizers/implementations\n",
      "  creating build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-3.6/tokenizers/tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for tokenizers\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Complete output from command /usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/tmp/job.19121190.hpc/pip-install-xdcpgga5/tokenizers/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" clean --all:\n",
      "  running clean\n",
      "  removing 'build/lib.linux-x86_64-3.6' (and everything under it)\n",
      "  'build/bdist.linux-x86_64' does not exist -- can't clean it\n",
      "  'build/scripts-3.6' does not exist -- can't clean it\n",
      "  removing 'build'\n",
      "  running clean_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed cleaning build dir for tokenizers\u001b[0m\n",
      "Failed to build tokenizers\n",
      "\u001b[31mtensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-gpu 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: regex, zipp, importlib-resources, tqdm, filelock, typing-extensions, importlib-metadata, pyyaml, packaging, huggingface-hub, numpy, tokenizers, dataclasses, joblib, click, sacremoses, transformers\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/gcc-6_3_0/openmpi-2_0_1/python/3.6.6/lib/python3.6/site-packages/regex-2023.8.8.dist-info'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mYou are using pip version 18.0, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0c41903ddb31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip install transformers '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "! pip install transformers \n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Initial HPC Dataset2.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedList = [x for x in data['Lyrics'] if str(x) != 'nan']\n",
    "cleanedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input= tokenizer(cleanedList, padding=True, truncation = True)\n",
    "print(encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
